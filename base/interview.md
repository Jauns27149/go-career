# 职业相关

## 自我介绍

​	面试官，您好！我叫吴恩强。我是一名goland开发的工程师，我熟悉Go的GPM调度模型和垃圾回收机制。在数据库方面，我经常使用Redis来处理缓存和提高系统性能，对它的数据类型、持久化机制以及缓存异常都比较了解。同时，我也熟悉MySQL这一类的关系型数据库，能够熟练地进行索引优化和事务管理，确保数据的一致性和完整性。我还熟悉一些主流的Go Web框架和技术，使用到Gin框架、GORM ORM库及go-redis等框架来快速构建Web程序。能熟练使用命令操作Linux和Docker，项目中使用到Docker Compose进行容器化部署。另外，我对常见的设计模式，如工厂模式和单例模式也有所了解，并在实际项目中有应用。我习惯使用Git进行版本控制，注重代码质量和团队协作。工作之余，个人十分喜欢Ubuntu系统，没事会去逛Guihub。最后，希望有机会加入贵公司。

## 为什么离职

​	自身的规划和公司发展方向不符，我想要往go方面发展，但是公司是主要业务是Java的。

## 你有什么要问我的吗？

### 技术面试

- 公司常用的技术栈是什么？
- 该岗位的日常工作是什么？有给我设定的特定目标吗？
- 我入职的岗位是新增还是接替之前离职的同事？（是否有技术债需要还）？
- 试用期结束的时候，你会怎么样衡量我的绩效？
- 工作是怎么组织的？团队有多大？公司技术团队的架构和人员组成？
- 产品 / 服务的规划是什么样的？（几周一次发布 / 持续部署 / 多个发布流 / ...)
- 入职培训会是什么样的？
- 有标准的开发环境吗？是强制的吗？

### 人事面试

- 绩效评估流程是怎样的？
- 公司的调薪制度是如何的？

- 办公室的布局如何？（开放的 / 小隔间 / 独立办公室）

- 有五险一金或者其他退休养老金等福利吗？
- 有什么医疗保险吗？如果有的话何时开始？

- 带薪休假时间有多久？
- 病假和事假是分开的还是一起算？
- 假期的更新策略是什么样的？也就是说未休的假期能否滚入下一周期？

## 目前有offer吗

你可以说：“刚开始找工作，您是我面试的第一家公司。”
你也可以这样说：“目前有2家公司在沟通，还没有发书面offer。”
如果你面试过很多家公司，但一直没有拿到offer，千万不要老实巴交的告诉面试官，应该不会有人这么做吧。

# Go基础

## 常用标准库的包

| 包名   | 作用                                                         |
| ------ | ------------------------------------------------------------ |
| net    | net包提供了可移植的网络I/O接口，包括TCP/IP、UDP、域名解析和Unix域socket |
| errors | errors包实现了创建错误值的函数                               |
| os     | os包提供了操作系统函数的不依赖平台的接口。设计为Unix风格的，虽然错误处理是go风格的；失败的调用会返回错误值而非错误码。 |
| sync   | sync包提供了基本的同步基元                                   |
| time   | time包提供了时间的显示和测量用的函数。日历的计算采用的是公历。 |
| io     | io包提供了对I/O原语的基本接口。本包的基本任务是包装这些原语已有的实现（如os包里的原语），使之成为共享的公共接口，这些公共接口抽象出了泛用的函数并附加了一些相关的原语的操作。 |
| fmt    | fmt包实现了类似C语言printf和scanf的格式化I/O。格式化动作（'verb'）源自C语言但更简单。 |
| log    | log包实现了简单的日志服务                                    |

## 数组和切片的区别

|      | 数组 Arrays      | 切片 Slices                            |
| ---- | ---------------- | -------------------------------------- |
| 声明 | 数组需要指定大小 | 不用指定，长度是不固定的，可以追加元素 |
| 类型 | 值类型           | 引用类型                               |

## new() 和 make() 的区别

|          | make(T, args)                 | new(T)                         |
| -------- | ----------------------------- | ------------------------------ |
| 返回值   | 初始化的T类型的值             | T 类型的指针（*T）             |
| 使用类型 | slice map channle（引用类型） | 数组 结构体（值类型 引用类型） |

注：new(T)和 make(T, args) 都是Go语言的内建函数，用来分配内存，但是适用类型不同。

## 结构体的比较

- 当 struct 类型相同，且所有字段都可以比较时，可以通过 != 或 = 进行比较
- 结构体只能比较是否相等，不能比较大小。
- 是否相等和属性类型、个数、顺序相关。

注：

1. 可以比较类型（ bool、值类型、string、指针、数组等），不可比较 （切片、数组、函数等）
2. 如果需要对循环类型比较，可以使用 reflect.DeepEqual(x, y any).

## nil 可以赋值的类型

​	指针、chan、函数、map、接口、切片。

## init() 函数

- 用于程序执行前做包的初始化的函数。
- 一个包可以有多个 init() 函数，按顺序执行。
- 不能被调用、引用。
- 包可以被循环导入，但是不能出现死循环，否则编译失败，且 init() 函数只会执行一次。

- 

## 获取map 中不存的值的结果

​	返回 map 中 value 的默认零值。

## for 和 for range 的区别

​	`for` 循环适用于需要更精细控制循环逻辑的情况，如复杂的条件判断或索引控制；`for range` 主要用于遍历集合类型的对象，简化代码。

## GPM

### 什么是Goroutine

​	goroutine 是一种轻量级的并发执行单元，由Go语言运行时自动管理和调度，而非由操作系统直接管理。

### 什么是 GPM

​	M-P-G模型，Go语言在运行时管理goroutine执行时使用到的一种调度机制。

- G：Goroutine,实际上我们每次调用go func 就是生成了一个G

- P：Processor，处理器，一般P的数量就是处理器的核数，可以通过GOMAXPROCS进行修改。

- M：Machine thread，系统线程，用来执行G.

  ​	在 GPM 模型，有一个全局队列（Global Queue）：存放等待运行的 G，还有一个 P 的本地队列：也是存放等待运行的 G，但数量有限，不超过 256 个

### 进程、线程、协程有什么区别

- 进程：是应用程序的启动实例，每个进程都有独立的内存空间。
- 线程：从属于进程，每个进程至少包含一个线程，线程是 CPU 调度的基本单位。
- 协程：轻量级的并发执行单元，由Go语言运行时自动管理和调度。

### 调度流程

1. **创建 Goroutine**：
    - 当通过 `go func()` 创建新的 Goroutine 时，G 会首先被加入到与当前 P 关联的本地队列中。
    - 如果 P 的本地队列已满（超过 256 个 G），则新的 G 会被放入全局队列。
2. **调度与执行**：
    - 每个 M 会与一个 P 绑定，M 从 P 的本地队列中获取一个 G 来执行。
    - 如果 P 的本地队列为空，M 会尝试从全局队列或其他 P 的本地队列中偷取（work stealing）任务执行。
3. **系统调用与阻塞**：
    - 当 G 执行过程中发生阻塞或系统调用，M 也会被阻塞。这时，P 会解绑当前的 M，并尝试寻找或创建新的 M 来继续执行其他 G。
    - 阻塞结束后，原来的 M 会尝试重新绑定一个 P 继续执行。

### G,P,M 的个数问题

- **G（Goroutine）的个数**
- **理论上无限制**：G的数量在理论上是没有上限的，只要系统的内存足够，就可以创建大量的goroutine。这是因为goroutine比线程更轻量级，它们共享相同的地址空间，并且在堆上分配的内存相对较少。
  
- **实际受内存限制**：尽管理论上goroutine的数量没有限制，但实际上它们会受到系统可用内存的限制。每个goroutine都需要分配一定的栈空间（尽管栈的大小可以动态调整），而且goroutine之间共享的数据结构（如全局变量、通道等）也会占用内存。


- **P（Processor）的个数**
- **通常设置为逻辑CPU数的两倍**：P的数量通常建议设置为逻辑CPU核心数的两倍，这是为了提高调度的并行性和效率。每个P都可以绑定到一个M上执行goroutine，而设置更多的P可以使得在某些M阻塞时，其他M仍然可以执行P上的goroutine，从而减少等待时间。
  
- **由GOMAXPROCS决定**：P的实际数量由环境变量`GOMAXPROCS`（或在Go程序中通过`runtime.GOMAXPROCS`函数设置）决定。这个值限制了同时运行的goroutine的数量，即在任何给定时间，最多只有`GOMAXPROCS`个goroutine在CPU上执行。


- **M（Machine/Thread）的个数**
- **动态创建和销毁**：M的数量是动态变化的，Go运行时根据需要创建和销毁M。当一个M上的所有goroutine都阻塞时，该M可能会被销毁，而当有goroutine等待执行但没有可用的M时，会创建新的M。
  
- **默认和最大限制**：Go程序启动时，会设置一个M的最大数量（默认通常是10000，但这个值可能因Go版本和操作系统而异），但这个限制很少达到，因为操作系统本身就有线程/进程数量的限制。此外，通过`runtime/debug`包中的`SetMaxThreads`函数可以设置M的最大数量，但这个函数主要用于调试目的，不建议在生产环境中随意更改。
  
- **与P的关系**：M与P之间没有绝对的固定关系。一个M可以绑定到任意P上执行goroutine，而当M阻塞时，它会释放其绑定的P，P随后会尝试绑定到其他空闲的M上。因此，即使P的数量较少，也可能因为工作量窃取和M的动态创建而有大量的M存在（尽管这些M中的大多数可能在等待中）。

### 关键机制

- **work stealing（工作量窃取） 机制**：会优先从全局队列里进行窃取，之后会从其它的P队列里窃取一半的G，放入到本地P队列里。
- **hand off （移交）机制**：M 被阻塞时，P 会被移交给其他空闲的 M，或者创建新的 M 来执行任务。

### 死锁场景

​	死锁是当 Goroutine被阻塞而无法解除阻塞时产生的一种状态。

**空读或多个协程互相等待**：

- 如果一个协程试图从一个它认为应该有数据的通道读取数据，但实际上通道是空的，这会导致协程阻塞。
- 如果多个协程互相依赖对方的动作来继续执行，而它们都在等待对方，也会导致死锁



## CG

​	goCG优化迭代过程

- GoV1.3- 普通标记清除法，整体过程需要启动STW，效率极低。
- GoV1.5- 三色标记法， 堆空间启动写屏障，栈空间不启动，全部扫描之后，需要重新扫描一次栈(需要STW)，效率普通
- GoV1.8-三色标记法，混合写屏障机制， 栈空间不启动，堆空间启动。整个过程几乎不需要STW，效率较高。

### 三色标记法 + 混合写屏障

- 颜色

    - 白色：尚未被处理的对象，可能成为垃圾。
    - 灰色：已经被发现但其子对象还未完全扫描的对象。

    - 黑色：已经完全扫描过，并且所有可达对象都已被标记的对象。

- 屏障
    - 插入写屏障：当一个新对象被创建或者现有对象被更新时，如果这个对象是从黑色对象指向白色对象，则需要将白色对象标记为灰色，确保它不会被误认为垃圾。
    - 删除写屏障：当一个黑色对象到另一个对象的指针被移除时，如果目标对象仅通过这一个指针可达，且该对象为白色时，那么需要将该对象标记为灰色。
- 规则
    - GC开始将栈上的对象全部扫描并标记为灰色(之后不再进行第二次重复扫描，无需STW)。
    - GC期间，任何在栈上创建的新对象，均为黑色。
    - 被删除的对象标记为灰色。
    - 被添加的对象标记为灰色。

- 流程

    1. 初始标记阶段：GC开始时会进行一次STW（Stop-The-World）暂停，以确保所有根对象（包括全局变量和栈上的指针）都被扫描并标记为灰色，堆空间对象为白色。然后从这些灰色对象开始，递归地将它们指向的对象也标记为相应的颜色。
    2. 并发标记阶段：在此期间，应用程序可以继续运行。任何新创建的对象默认是白色的。如果一个白色对象被黑色对象引用，它会被写屏障机制捕获，并且该对象会被重新标记为灰色，以便稍后被处理。
    3. 写屏障
        - 插入写屏障：当一个黑色对象引用了一个新的白色对象时，这个白色对象必须被标记为灰色，防止其被错误地当作垃圾回收。
        - 删除写屏障：当一个黑色对象到另一个对象的指针被移除时，如果目标对象仅通过这一个指针可达，那么需要做额外的记录以防止其被错误地当作垃圾回收。

    4. 清扫：接下来进入清扫阶段，回收所有未被标记的白色对象所占用的空间。

### GC 的触发时机？

1. gcTriggerHeap：当所分配的堆大小达到阈值（由控制器计算的触发堆的大小）时，将会触发。

2. gcTriggerTime：当距离上一个 GC 周期的时间超过一定时间时，将会触发。时间周期以runtime.forcegcperiod 变量为准，默认 2 分钟。

3. gcTriggerCycle：如果定期没有开启 GC，则启动 GC。

4. 手动触发的 runtime.GC 方法。



## context

### context是什么

​	它是 goroutine 的上下文，包含 goroutine 的运行状态、环境、现场等信息。context 主要用来在 goroutine 之间传递上下文信息，包括：取消信号、超时时间、截止时间、k-v 等。

### context的方法

- `Deadline`：返回的第一个值是 **截止时间**，到了这个时间点，Context 会自动触发 Cancel 动作。返回的第二个值是 一个布尔值，true 表示设置了截止时间，false 表示没有设置截止时间，如果没有设置截止时间，就要手动调用 cancel 函数取消 Context。
- `Done`：返回一个只读的通道（只有在被cancel后才会返回），类型为 `struct{}`。当这个通道可读时，意味着parent context已经发起了取消请求，根据这个信号，开发者就可以做一些清理动作，退出goroutine。
- `Err`：返回 context 被 cancel 的原因。
- `Value`：返回被绑定到 Context 的值，是一个键值对，所以要通过一个Key才可以获取对应的值，这个值一般是线程安全的。

## 内存

### 内存泄露

内存泄漏（Memory Leak）是指程序中未正确释放已分配的内存，导致内存逐渐被耗尽，最终可能导致程序崩溃或系统性能下降。

#### 泄露场景

- 长生命周期的对象引用：如果一个对象被意外地保持引用，即使它不再需要使用，也无法被垃圾回收器回收。
    - 例如
        - 将对象放入全局变量
        - 长生命周期的容器（如切片、映射）
        - 通过闭包捕获引用
- 忘记关闭资源：打开文件、数据库连接、网络连接等资源未被及时关闭，会导致相应的内存资源无法被释放。

#### 排查内存泄露情况

pprof 是 Go 自带的性能分析工具，可以用来分析 CPU、内存、goroutine、块和线程创建等情况。它可以帮助你识别内存泄漏。

1. 使用`net/http/pprof`,监控程序

```go
package main

import (
    _ "net/http/pprof"
)

......

func main() {
    go func() {
        http.ListenAndServe("localhost:6060", nil)
    }()
......
}
```

2. 运行程序,生成内存分析数据

```bash
curl -o mem.prof http://localhost:6060/debug/pprof/heap
```

3. 使用 `pprof` 查看内存分析报告

```bash
go tool pprof mem.prof
```

### 内存逃逸

​	内存逃逸是指原本可以在栈上分配的变量被分配到了堆上。

#### 逃逸场景

- 指针逃逸: 函数返回一个局部变量的指针
- 栈空间不足逃逸: 当栈空间不足以存放当前对象时或无法判断当前切片长度时会将对象分配到堆中
- 动态类型逃逸: 局部变量作为interface类型传参给函数
- 闭包引用对象逃逸：闭包函数捕获了外部函数的局部变量

#### 什么是逃逸分析

​	逃逸分析（Escape analysis）是指由编译器决定内存分配的位置，不需要程序员指定. 程序员可以通过go的工具参数`-gcflags=-m`查看逃逸情况.

## GRPC

​	GRPC是google开源的一个基于protobuf的高性能、跨语言的RPC框架。

### protocol buffers

​	Protobuf（Protocol Buffers）是 Google 开发的一种数据序列化工具。它类似于 XML，但比 XML 更小、更快、更简单。

## Gin

### 特性

1. **快速**
    1. 基于 Radix 树的路由，小内存占用。没有反射。可预测的 API 性能。
2. **支持中间件**
    1. 传入的 HTTP 请求可以由一系列中间件和最终操作来处理。 例如：Logger，Authorization，GZIP，最终操作 DB。
3. **Crash 处理**
    1. Gin 可以 catch 一个发生在 HTTP 请求中的 panic 并 recover 它。这样，你的服务器将始终可用。例如，你可以向 Sentry 报告这个 panic！
4. **JSON 验证**
    1. Gin 可以解析并验证请求的 JSON，例如检查所需值的存在。
5. **路由组**
    1. 更好地组织路由。是否需要授权，不同的 API 版本…… 此外，这些组可以无限制地嵌套而不会降低性能。
    2. Gin 使用基于树状结构的路由匹配算法，能够快速地匹配 URL 路径
6. **错误管理**
    1. Gin 提供了一种方便的方法来收集 HTTP 请求期间发生的所有错误。最终，中间件可以将它们写入日志文件，数据库并通过网络发送。
7. **内置渲染**
    1. Gin 为 JSON，XML 和 HTML 渲染提供了易于使用的 API。
8. **可扩展性**
    1. 新建一个中间件非常简单，去查看[示例代码](https://gin-gonic.com/zh-cn/docs/examples/using-middleware/)吧。

###  请求打入到响应的一个过程

**Gin 框架的请求处理过程大致分为以下几个步骤：**

1. **请求接收**：
   当 HTTP 请求到达 Gin 应用时，Gin 框架会首先接收到请求。这些请求会被 `*gin.Engine` 对象处理，`Engine` 是 Gin 的核心组件。
2. **路由匹配**：
   Gin 根据请求的 URL 和 HTTP 方法（如 GET、POST）来匹配路由。框架会查找定义的路由规则，并找到与请求最匹配的处理函数（Handler）。
3. **中间件处理**：
   在执行路由处理函数之前，Gin 会依次执行与该路由关联的中间件。中间件可以用于请求的预处理，如认证、日志记录等。
4. **执行处理函数**：
   中间件执行完毕后，Gin 会调用匹配的路由处理函数。处理函数可以访问请求数据、处理业务逻辑，并准备响应数据。
5. **生成响应**：
   处理函数会通过 `*gin.Context` 对象生成响应。可以设置响应状态码、响应头以及响应体。Gin 提供了多种方法来构造响应，比如 `c.String()`、`c.JSON()`、`c.XML()` 等。
6. **响应返回**：
   最终，Gin 将响应数据发送回客户端，完成请求-响应周期。

**总结**：Gin 框架处理请求的流程从接收请求开始，经过路由匹配和中间件处理，执行处理函数，生成并返回响应。整个过程高效且结构清晰，帮助开发者快速构建 Web 应用。

## GORM

GORM 是一个强大的 Golang ORM（对象关系映射）库，它能够简化数据库操作，使开发者能够通过 Golang 代码与数据库进行交互，而不需要直接编写 SQL 语句。GORM 支持自动映射数据库表结构到 Golang 结构体，并提供了丰富的链式调用方法来进行增删改查操作。
使用 GORM 时，我们可以通过结构体字段标签（例如 `gorm:"column:name"`）来指定数据库表的列名、数据类型、索引等。它还支持事务、预加载、关联关系（如一对一、一对多、多对多）等高级特性，适合构建复杂的业务系统。
在性能方面，GORM 的操作虽然较为直观和简洁，但它会带来一定的性能开销，特别是在处理大批量数据或高并发场景时，需要注意优化查询语句或选择适当的数据库操作方式，比如使用原生 SQL 语句。
总的来说，GORM 适合大多数应用场景，特别是对于中小型项目或者需要快速开发的项目来说，能显著提高开发效率。

## 数据库字段怎么进行映射

​	GORM 通过将 Go 结构体（Go structs） 映射到数据库表来简化数据库交互。

# MySQL

## 数据库的三范式是什么？

- 第一范式：保证每列的原子性，数据库表中的所有字段值都是不可分解的原子值。
- 第二范式：保证一张表只描述一件事情，所有非关键字段都完全依赖于任一组候选关键字。
- 第三范式：保证每列都和主键直接相关，表中的字段和主键直接对应不依靠其他中间字段。

## 存储引擎类型

### MySQL有哪些存储引擎

| 名称            | 概述                                   | 场景                                       |
| --------------- | -------------------------------------- | ------------------------------------------ |
| InnoDB(DEFAULT) | 事务型的存储引擎，有行级锁定和外键约束 | 事务处理应用程序                           |
| MyISAM          | 不支持事物，性能极佳                   | 非事务表，高速存储和检索，以及全文搜索能力 |
| ......          |                                        |                                            |

### InnoDB和MyISAM的区别

|          | InnoDB         | MyISAM     |
| -------- | -------------- | ---------- |
| 事务     | 支持           | 不支持     |
| 外键     | 支持           | 不支持     |
| 索引类型 | 聚簇索引       | 非聚簇索引 |
| 锁类型   | 行级锁         | 表级锁     |
| 主键     | 主键或唯一索引 | 可以没有   |
| .......  |                |            |

InnoDB如果没有设定主键或非空唯一索引，就会自动生成一个用户不可见6字节的主键

## 事务

### 什么是数据库事务？

​	数据库事务是一种数据库操作机制，它确保一组数据库操作要么全部成功执行，要么全部不执行。

### 事务的四个特征（ACID）

- 原子性（Atomicity）：所有操作被视为一个不可分割的工作单元，如果失败则回滚。
    - 例子：你从取款机取钱,这个事务可以分成两个步骤:1划卡,2出钱.不可能划了卡,而钱却没出来.这两步必须同时完成.要么就不完成。

- 一致性（Consistency）：指事务的运行并不改变数据库中数据的一致性。
    - 例子：完整性约束了a+b=10，一个事务改变了a，那么b也应该随之改变。

- 独立性（Isolation）：事务的独立性也有称作隔离性，是指两个以上的事务不会出现交错执行的状态，因为这样可能会导致数据不一致。
- 持久性（Durability）：事务的持久性是指事务执行成功以后，该事务所对数据库所作的更改便是持久的保存在数据库之中，不会无缘无故的回滚。

### 什么是脏读？幻读？不可重复读？

- 脏读：事务 A 读取了事务 B 更新的数据，然后 B 回滚操作，那么 A 读取到的数据是脏数据
- 不可重复读：事务 A 多次读取同一数据，事务 B 在事务 A 多次读取的过程中，对数据作了更新并提交，导致事务 A 多次读取同一数据时，结果 不一致。
- 幻读：系统管理员 A 将数据库中所有学生的成绩从具体分数改为 ABCDE 等级，但是系统管理员 B 就在这个时候插入了一条具体分数的记录，当系统管理员 A 改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。
  不可重复读侧重于修改，幻读侧重于新增或删除（多了或少量行），脏读是一个事务回滚影响另外一个事务。

### 说一下MySQL 的四种隔离级别

| 名称     | 英文             | 含义                                           | 问题                   |
| -------- | ---------------- | ---------------------------------------------- | ---------------------- |
| 读未提交 | Read Uncommitted | 所有事务都可以看到其他未提交事务的执行结果     | 脏读、不可重复读、幻读 |
| 读已提交 | Read Committed   | 一个事务只能看见已经提交事务所做的改变         | 不可重复读、幻读       |
| 可重复读 | Repeatable Read  | 确保在一个事务中多次读取同一数据的结果是一致的 | 幻读                   |
| 可串行化 | Serializable     | 通过强制事务排序，使之不可能相互冲突           | /                      |

注：MySQL默认隔离级别是可重复读（Repeatable Read）。

### MySQL事务的实现原理

事务是基于重做日志文件(redo log)和回滚日志(undo log)实现的。
每提交一个事务必须先将该事务的所有日志写入到重做日志文件进行持久化，数据库就可以通过重做日志来保证事务的原子性和持久性。
每当有修改事务时，还会产生 undo log，如果需要回滚，则根据 undo log 的反向语句进行逻辑操作，比如 insert 一条记录就 delete 一条记录。undo log 主要实现数据库的一致性。

## 索引

### 索引是什么? 索引优缺点?

- 索引类似于目录, 进行数据的快速定位
- 优点: 加快数据检索速度
- 缺点: 创建索引和维护索引需要消耗空间和时间

### MySQL索引分类

#### 按数据结构分类

- 树型数据结构索引：InnoDB存储引擎就是用B+Tree实现MySQL索引结构，B+树在叶子节点存储key和数据，非叶子节点只存储key不存储数据所以非叶子节点可以包含更多的节点，这样做可以降低树的高度。
- Hash数据结构索引：Hash表是一种key-value的存储结构，通过key计算出下标，然后将value添加到该下标对应的链表中去。

#### 按物理存储方式分类

- 聚簇索引（InnoDB引擎）：主键索引属于聚簇索引的叶子节点会存储指针的值和数据行，也就是说数据和索引是在一起。

- 非聚簇索引（MyISAM引擎）：二级索引（辅助索引）属于非聚簇索引，叶子节点只会存储数据行的指针，简单来说数据和索引不在一起。

#### 按索引字段的特性分类

- 普通索引：MySQL中最基本的索引类型，没有任何限制，允许定义索引的列中插入重复值和空值。
- 唯一索引：UNIQUE 表示唯一索引，与普通索引类似，用于实现唯一约束；索引列中的值必须是唯一的，但是允许空值；如果是组合索引，则列值的组合必须唯一，创建方法和普通索引类似。
- 主键索引：和唯一索引相似，但是不允许有NULL值
- 复合索引：在表中的多个字段组合上创建的索引，遵循最左前缀原则。
- 前缀索引：MySQL 允许基于 CHAR、VARCHAR、BINARY 以及 VARBINARY 字段的最前面一部分内容创建索引，同时对于 BLOB 和 TEXT 字段必须指定索引使用的字段长度。这一特性被称为前缀索引（prefix index）。
- 全文索引：全文索引是一种用于文本数据模糊查询的特殊索引，它基于倒排索引实现。

### 最左匹配原则

​	使用复合索引时在查询条件中使用了这些字段的左边字段时，索引才会被使用。

### B+Tree索引

**时间复杂度**：B+树的时间复杂度同样为O(log n)，但在某些情况下可能会表现为O(log m * log n)，其中m是节点的最大分支数。这种复杂度主要出现在考虑磁盘I/O操作的场景下。然而，在内存操作中，B+树的查询时间复杂度通常仍为O(log n)。
是B-Tree的改进版本，同时也是数据库索引索引所采用的存储结构。数据都在叶子节点上，并且增加了顺序访问指针，每个叶子节点都指向相邻的叶子节点的地址。相比B-Tree来说，进行范围查找时只需要查找两个节点，进行遍历即可。而B-Tree需要获取所有节点，相比之下B+Tree效率更高。
B+tree性质：

- n棵子tree的节点包含n个关键字，不用来保存数据而是保存数据的索引。
- 所有的叶子结点中包含了全部关键字的信息，及指向含这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。
- 所有的非终端结点可以看成是索引部分，结点中仅含其子树中的最大（或最小）关键字。
- B+ 树中，数据对象的插入和删除仅在叶节点上进行。
- B+树有2个头指针，一个是树的根节点，一个是最小关键码的叶节点。

![image.png](https://cdn.nlark.com/yuque/0/2022/png/22219483/1647160383306-24a883eb-838d-4dd3-9bfe-d3b7c73fa3e8.png#averageHue=%23f7f7f7&clientId=u3111ec8c-f05a-4&from=paste&id=u32a7c857&originHeight=359&originWidth=780&originalType=url&ratio=1&rotation=0&showTitle=false&size=63304&status=done&style=none&taskId=u6c3a8d80-0b06-4dee-8b28-b2f5cf3e52b&title=)

### 建立索引的原则

- 建立索引的字段最好为NOT NULL
- 索引字段占用空间越小越好
- 最左匹配原则
- =和in建立索引时顺序可以任意, 比如a = 1 and b = 2 and c = 3 建立(a, b, c)和(b, a, c)索引效果是一样的, MySQL查询优化器会进行优化
- 建立的索引让索引的选择性尽可能接近1, 唯一索引的索引选择性为1
- 尽量扩展索引, 不要让索引冗余, 如有SQL需要对单个a进行索引, 那么上述条件建立的索引应该为(a, b, c)或(a, c, b)
- 索引列不能参与计算

### 索引失效场景

- 对索引使用左或者左右模糊匹配
- 对索引使用函数
- 对索引进行表达式计算
- 对索引隐式类型转换
- 联合索引非最左匹配
- WHERE 子句中的 OR

### 覆盖索引

​	查询的数据完全包含在索引的列中时，MySQL 可以直接从索引中获取数据，而无需再回表查询主键索引，从而减少磁盘 I/O 操作。



### mysql一般你有什么调优的经验

在 MySQL 中进行性能调优的经验涉及多个方面，从数据库设计到查询优化，再到服务器配置等。以下是一些常见的 MySQL 调优技巧：

### 1. 数据库设计

- **规范化**：确保数据库表结构设计良好，避免数据冗余。
- **反规范化**：在某些读多写少的场景下，适度的反规范化可以减少复杂查询，提高性能。
- **适当的字段类型**：选择合适的数据类型，尽量使用定长类型，例如使用 `INT` 而不是 `VARCHAR` 存储数字。

### 2. 索引优化

- **创建合适的索引**：为频繁查询的字段创建索引，特别是主键、外键和需要排序或分组的字段。
- **覆盖索引**：通过索引覆盖查询来减少数据读取，例如 `SELECT id, name FROM users WHERE id = ?`，如果有 `id` 和 `name` 的联合索引，就可以直接从索引中获取数据而不需要读取表。
- **避免冗余索引**：删除重复或不必要的索引，减少索引维护开销。
- **复合索引**：为多列创建复合索引，可以有效加速涉及多列的查询。

### 3. 查询优化

- **EXPLAIN 分析查询**：使用 `EXPLAIN` 关键字来分析查询执行计划，找出性能瓶颈。
- **避免全表扫描**：确保查询条件使用了索引，避免全表扫描。
- **适当使用 JOIN 和子查询**：优化 JOIN 和子查询的使用，避免复杂的嵌套查询。
- **分页优化**：对于大数据量分页查询，避免使用 `OFFSET`，可以使用延迟关联或子查询来优化。

### 4. 缓存和临时表

- **查询缓存**：MySQL 支持查询缓存，但在高并发环境下可能效果不佳，可以使用应用层缓存，如 Redis。
- **临时表**：在复杂查询中，适当使用临时表可以分解查询，提高性能。

### 5. 配置优化

- **InnoDB 引擎配置**：
    - `innodb_buffer_pool_size`：设置为物理内存的 70%-80%，用于缓存数据和索引。
    - `innodb_log_file_size`：适当调整日志文件大小，提高写入性能。
    - `innodb_flush_log_at_trx_commit`：根据需求调整为 0、1 或 2，以平衡数据安全和性能。
- **查询缓存配置**：在 MySQL 8.0 中已被废弃，建议使用应用层缓存解决方案。
- **连接数配置**：调整 `max_connections` 和 `thread_cache_size`，根据并发需求设置合适的值。

### 6. 硬件和系统优化

- **磁盘 I/O 优化**：使用 SSD 替代 HDD，提高读写性能。
- **内存优化**：增加服务器内存，确保数据库有足够的缓存空间。
- **CPU 优化**：使用多核 CPU，确保 MySQL 可以充分利用多核性能。

# Redis

## 数据类型

| 类型             | 概述                                                        |
| ---------------- | ----------------------------------------------------------- |
| String（字符串） | 键值对存储                                                  |
| List（列表）     | 字符串列表，按照插入顺序排序                                |
| Set（集合）      | 无序且不重复的字符串集合                                    |
| Zset（有序集合） | 集合，但每个成员关联了一个分数（score），用来对成员进行排序 |
| Hash（散列）     | 键值对的映射表，每个哈希可以包含多个键值对。                |

## 持久化

### Redis持久化机制

​	为了能够重用Redis数据，或者防止系统故障，我们需要将Redis中的数据写入到磁盘空间中，即持久化。
Redis提供了两种不同的持久化方法可以将数据存储在磁盘中，一种叫快照RDB，另一种叫只追加文件AOF。

- RDB：在指定的时间间隔内将内存中的数据集快照写入磁盘(Snapshot)，它恢复时是将快照文件直接读到内存里。

    - 优势：

        - 只有一个文件（dump.rdb），持久化方便
        - 容灾性好，一个文件可以安全保存到磁盘

        - 性能最大化：fork**子进程**来完成写操作，主进程继续处理命令，所以是IO最大化。（主进程不涉及IO处理，子进程单独处理持久化，保证了Redis的高性能）

        - 数据集较大时，启动效率更高（相比于AOF）


- 劣势：

    - 在一定间隔时间做一次备份，所以如果Redis意外down掉的话，就会丢失最后一次快照后的所有修改。

    - 数据安全性低：隔一段时间进行持久化的机制，如果发生故障容易导致数据丢失

    - 适用于数据要求不严谨的情况下


- AOF：AOF( append only file )持久化以独立日志的方式记录每次写命令，并在 Redis 重启时在重新执行 AOF 文件中的命令以达到恢复数据的目的。AOF 的主要作用是解决数据持久化的实时性。
  以日志的形式来记录每个写操作，将Redis执行过的所有写指令记录下来(读操作不记录)，只许追加文件但不可以改写文件，Redis启动之初会读取该文件重新构建数据，换言之，Redis重启的话就根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作。
  AOF采用文件追加方式，文件会越来越大，为避免出现此种情况，新增了重写机制，当AOF文件的大小超过所设定的阈值时， Redis就会启动AOF文件的内容压缩，只保留可以恢复数据的最小指令集.。

    - 劣势

        - 相同数据集的数据而言aof文件要远大于rdb文件，恢复速度慢于rdb

        - aof运行效率要慢于rdb，每秒同步策略效率较好，不同步效率和rdb相同


- AOF文件同步的三种机制

    - 每修改同步always：同步持久化，每次发生数据变更会被立即记录到磁盘，性能较差但数据完整性比较好

    - 每秒同步everysec：异步操作，每秒记录，如果一秒内宕机，有数据丢失
    - 不同no：从不同步


### 如何选择合适的持久化方式

- 如果是数据不那么敏感，且可以从其他地方重新生成补回的，那么可以关闭持久化。
- 如果是数据比较重要，不想再从其他地方获取，且可以承受数分钟的数据丢失，比如缓存等，那么可以只使用RDB。
- 如果是用做内存数据库，要使用Redis的持久化，建议是RDB和AOF都开启，或者定期执行bgsave做快照备份，RDB方式更适合做数据的备份，AOF可以保证数据的不丢失。

## 缓存异常

缓存异常有四种类型，分别是缓存和数据库的数据不一致、缓存雪崩、缓存击穿和缓存穿透。

###  如何保证缓存与数据库双写时的数据一致性？（重要）

背景：使用到缓存，无论是本地内存做缓存还是使用 Redis 做缓存，那么就会存在数据同步的问题，因为配置信息缓存在内存中，而内存时无法感知到数据在数据库的修改。这样就会造成数据库中的数据与缓存中数据不一致的问题。
共有四种方案：

1. 先更新数据库，后更新缓存
2. 先更新缓存，后更新数据库
3. **先删除缓存，后更新数据库**
4. **先更新数据库，后删除缓存**

第一种和第二种方案，没有人使用的，因为第一种方案存在问题是：并发更新数据库场景下，会将脏数据刷到缓存。
第二种方案存在的问题是：如果先更新缓存成功，但是数据库更新失败，则肯定会造成数据不一致。
目前主要用第三和第四种方案。

#### 先删除缓存，后更新数据库

该方案也会出问题，此时来了两个请求，请求 A（更新操作） 和请求 B（查询操作）

1. 请求A进行写操作，删除缓存
2. 请求B查询发现缓存不存在
3. 请求B去数据库查询得到旧值
4. 请求B将旧值写入缓存
5. 请求A将新值写入数据库

上述情况就会导致不一致的情形出现。而且，如果不采用给缓存设置过期时间策略，该数据永远都是脏数据。

##### 答案一：延时双删

最简单的解决办法延时双删
使用伪代码如下：
public void write(String key,Object data){
Redis.delKey(key);
        db.updateData(data);
        Thread.sleep(1000);
        Redis.delKey(key);
}
转化为中文描述就是（1）先淘汰缓存（2）再写数据库（这两步和原来一样）（3）休眠1秒，再次淘汰缓存，这么做，可以将1秒内所造成的缓存脏数据，再次删除。确保读请求结束，写请求可以删除读请求造成的缓存脏数据。自行评估自己的项目的读数据业务逻辑的耗时，写数据的休眠时间则在读数据业务逻辑的耗时基础上，加几百ms即可。
如果使用的是 Mysql 的读写分离的架构的话，那么其实主从同步之间也会有时间差。
![](https://cdn.nlark.com/yuque/0/2022/webp/22219483/1647160426605-a00af22b-8a57-4225-a7a8-4609e1b13c75.webp#averageHue=%23f8f8f5&clientId=u9081c6a5-99f0-4&from=paste&id=u44093d42&originHeight=722&originWidth=1200&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u09bbe8ec-c9a3-4996-a8f8-f356fa7e1b4&title=)
此时来了两个请求，请求 A（更新操作） 和请求 B（查询操作）

1. 请求 A 更新操作，删除了 Redis
2. 请求主库进行更新操作，主库与从库进行同步数据的操作
3. 请 B 查询操作，发现 Redis 中没有数据
4. 去从库中拿去数据
5. 此时同步数据还未完成，拿到的数据是旧数据

此时的解决办法就是如果是对 Redis 进行填充数据的查询数据库操作，那么就强制将其指向主库进行查询。
![](https://cdn.nlark.com/yuque/0/2022/webp/22219483/1647160426602-805271f5-42fa-4b51-9870-74ce3d1f5165.webp#averageHue=%23f9f9f7&clientId=u9081c6a5-99f0-4&from=paste&id=u9f70511b&originHeight=890&originWidth=1456&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=uaa3b1a83-9495-42e4-8f3e-c287f91c193&title=)

##### 答案二： **更新与读取操作进行异步串行化**

采用**更新与读取操作进行异步串行化**
**异步串行化**
我在系统内部维护n个内存队列，更新数据的时候，根据数据的唯一标识，将该操作路由之后，发送到其中一个jvm内部的内存队列中（对同一数据的请求发送到同一个队列）。读取数据的时候，如果发现数据不在缓存中，并且此时队列里有更新库存的操作，那么将重新读取数据+更新缓存的操作，根据唯一标识路由之后，也将发送到同一个jvm内部的内存队列中。然后每个队列对应一个工作线程，每个工作线程串行地拿到对应的操作，然后一条一条的执行。
这样的话，一个数据变更的操作，先执行删除缓存，然后再去更新数据库，但是还没完成更新的时候，如果此时一个读请求过来，读到了空的缓存，那么可以先将缓存更新的请求发送到队列中，此时会在队列中积压，排在刚才更新库的操作之后，然后同步等待缓存更新完成，再读库。
**读操作去重**
多个读库更新缓存的请求串在同一个队列中是没意义的，因此可以做过滤，如果发现队列中已经有了该数据的更新缓存的请求了，那么就不用再放进去了，直接等待前面的更新操作请求完成即可，待那个队列对应的工作线程完成了上一个操作（数据库的修改）之后，才会去执行下一个操作（读库更新缓存），此时会从数据库中读取最新的值，然后写入缓存中。
如果请求还在等待时间范围内，不断轮询发现可以取到值了，那么就直接返回；如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的旧值。（返回旧值不是又导致缓存和数据库不一致了么？那至少可以减少这个情况发生，因为等待超时也不是每次都是，几率很小吧。这里我想的是，如果超时了就直接读旧值，这时候仅仅是读库后返回而不放缓存）

#### 先更新数据库，后删除缓存

这一种情况也会出现问题，比如更新数据库成功了，但是在删除缓存的阶段出错了没有删除成功，那么此时再读取缓存的时候每次都是错误的数据了。
![](https://cdn.nlark.com/yuque/0/2022/webp/22219483/1647160426387-4300eade-a278-4dcc-8045-e4554eab8691.webp#averageHue=%23f5f5f3&clientId=u9081c6a5-99f0-4&from=paste&height=480&id=u70a95ba6&originHeight=408&originWidth=643&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=udb6b5e67-2d32-495c-a7d3-f96afd05d25&title=&width=757)
此时解决方案就是利用消息队列进行删除的补偿。具体的业务逻辑用语言描述如下：

1. 请求 A 先对数据库进行更新操作
2. 在对 Redis 进行删除操作的时候发现报错，删除失败
3. 此时将Redis 的 key 作为消息体发送到消息队列中
4. 系统接收到消息队列发送的消息后再次对 Redis 进行删除操作

但是这个方案会有一个缺点就是会对业务代码造成大量的侵入，深深的耦合在一起，所以这时会有一个优化的方案，我们知道对 Mysql 数据库更新操作后再 binlog 日志中我们都能够找到相应的操作，那么我们可以订阅 Mysql 数据库的 binlog 日志对缓存进行操作。
![](https://cdn.nlark.com/yuque/0/2022/webp/22219483/1647160426352-d06f8f25-a24a-4c18-bd62-337f66d8a8cb.webp#averageHue=%23f4f4f0&clientId=u9081c6a5-99f0-4&from=paste&height=631&id=u0ca02646&originHeight=388&originWidth=518&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u0cd0d84c-2a4c-41a6-890a-3e5c1d76840&title=&width=843)

### 什么是缓存击穿?

缓存击穿跟缓存雪崩有点类似，缓存雪崩是大规模的key失效，而缓存击穿是某个热点的key失效，大并发集中对其进行请求，就会造成大量请求读缓存没读到数据，从而导致高并发访问数据库，引起数据库压力剧增。这种现象就叫做**缓存击穿**。
从两个方面解决，第一是否可以考虑热点key不设置过期时间，第二是否可以考虑降低在数据库上的请求数量。
解决方案：

- 在缓存失效后，通过互斥锁或者队列来控制读数据写缓存的线程数量，比如某个key只允许一个线程查询数据和写缓存，其他线程等待。这种方式会阻塞其他的线程，此时系统的吞吐量会下降
- 热点数据缓存永远不过期。永不过期实际包含两层意思：
    - 物理不过期，针对热点key不设置过期时间
    - 逻辑过期，把过期时间存在key对应的value里，如果发现要过期了，通过一个后台的异步线程进行缓存的构建

### 什么是缓存穿透?

缓存穿透是指用户请求的数据在缓存中不存在即没有命中，同时在数据库中也不存在，导致用户每次请求该数据都要去数据库中查询一遍。如果有恶意攻击者不断请求系统中不存在的数据，会导致短时间大量请求落在数据库上，造成数据库压力过大，甚至导致数据库承受不住而宕机崩溃。
缓存穿透的关键在于在Redis中查不到key值，它和缓存击穿的根本区别在于传进来的key在Redis中是不存在的。假如有黑客传进大量的不存在的key，那么大量的请求打在数据库上是很致命的问题，所以在日常开发中要对参数做好校验，一些非法的参数，不可能存在的key就直接返回错误提示。
![image.png](https://cdn.nlark.com/yuque/0/2022/png/22219483/1647160427139-03e58aa4-1001-4b6f-807f-217a884c2069.png#averageHue=%23f1f1f1&clientId=u9081c6a5-99f0-4&from=paste&id=ud9cc69a8&originHeight=291&originWidth=1005&originalType=url&ratio=1&rotation=0&showTitle=false&size=50073&status=done&style=none&taskId=u049ca7bc-b3a2-44a4-a39d-2efbaf2135d&title=)
解决方法：

- 将无效的key存放进Redis中：

当出现Redis查不到数据，数据库也查不到数据的情况，我们就把这个key保存到Redis中，设置value="null"，并设置其过期时间极短，后面再出现查询这个key的请求的时候，直接返回null，就不需要再查询数据库了。但这种处理方式是有问题的，假如传进来的这个不存在的Key值每次都是随机的，那存进Redis也没有意义。

- 使用布隆过滤器：

如果布隆过滤器判定某个 key 不存在布隆过滤器中，那么就一定不存在，如果判定某个 key 存在，那么很大可能是存在(存在一定的误判率)。于是我们可以在缓存之前再加一个布隆过滤器，将数据库中的所有key都存储在布隆过滤器中，在查询Redis前先去布隆过滤器查询 key 是否存在，如果不存在就直接返回，不让其访问数据库，从而避免了对底层存储系统的查询压力。
如何选择：针对一些恶意攻击，攻击带过来的大量key是随机，那么我们采用第一种方案就会缓存大量不存在key的数据。那么这种方案就不合适了，我们可以先对使用布隆过滤器方案进行过滤掉这些key。所以，针对这种key异常多、请求重复率比较低的数据，优先使用第二种方案直接过滤掉。而对于空数据的key有限的，重复率比较高的，则可优先采用第一种方式进行缓存。

### 什么是缓存雪崩?

如果缓在某一个时刻出现大规模的key失效，那么就会导致大量的请求打在了数据库上面，导致数据库压力巨大，如果在高并发的情况下，可能瞬间就会导致数据库宕机。这时候如果运维马上又重启数据库，马上又会有新的流量把数据库打死。这就是**缓存雪崩**。
造成缓存雪崩的关键在于同一时间的大规模的key失效，主要有两种可能：第一种是Redis宕机，第二种可能就是采用了相同的过期时间。
解决方案：
1、**事前**：

- 均匀过期：设置不同的过期时间，让缓存失效的时间尽量均匀，避免相同的过期时间导致缓存雪崩，造成大量数据库的访问。如把每个Key的失效时间都加个随机值，setRedis（Key，value，time + Math.random() * 10000）；，保证数据不会在同一时间大面积失效。
- 分级缓存：第一级缓存失效的基础上，访问二级缓存，每一级缓存的失效时间都不同。
- 热点数据缓存永远不过期。永不过期实际包含两层意思：
    - 物理不过期，针对热点key不设置过期时间
    - 逻辑过期，把过期时间存在key对应的value里，如果发现要过期了，通过一个后台的异步线程进行缓存的构建
- 保证Redis缓存的高可用，防止Redis宕机导致缓存雪崩的问题。可以使用 主从+ 哨兵，Redis集群来避免 Redis 全盘崩溃的情况。

2、**事中**：

- 互斥锁：在缓存失效后，通过互斥锁或者队列来控制读数据写缓存的线程数量，比如某个key只允许一个线程查询数据和写缓存，其他线程等待。这种方式会阻塞其他的线程，此时系统的吞吐量会下降
- 使用熔断机制，限流降级。当流量达到一定的阈值，直接返回“系统拥挤”之类的提示，防止过多的请求打在数据库上将数据库击垮，至少能保证一部分用户是可以正常使用，其他用户多刷新几次也能得到结果。

3、**事后**：
开启Redis持久化机制，尽快恢复缓存数据，一旦重启，就能从磁盘上自动加载数据恢复内存中的数据。

### 什么是缓存预热?

缓存预热是指系统上线后，提前将相关的缓存数据加载到缓存系统。避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题，用户直接查询事先被预热的缓存数据。
如果不进行预热，那么Redis初始状态数据为空，系统上线初期，对于高并发的流量，都会访问到数据库中， 对数据库造成流量的压力。
缓存预热解决方案：

- 数据量不大的时候，工程启动的时候进行加载缓存动作；
- 数据量大的时候，设置一个定时任务脚本，进行缓存的刷新；
- 数据量太大的时候，优先保证热点数据进行提前加载到缓存。

### 什么是缓存降级？

缓存降级是指缓存失效或缓存服务器挂掉的情况下，不去访问数据库，直接返回默认数据或访问服务的内存数据。降级一般是有损的操作，所以尽量减少降级对于业务的影响程度。
在进行降级之前要对系统进行梳理，看看系统是不是可以丢卒保帅；从而梳理出哪些必须誓死保护，哪些可降级；比如可以参考日志级别设置预案：

- 一般：比如有些服务偶尔因为网络抖动或者服务正在上线而超时，可以自动降级；
- 警告：有些服务在一段时间内成功率有波动（如在95~100%之间），可以自动降级或人工降级，并发送告警；
- 错误：比如可用率低于90%，或者数据库连接池被打爆了，或者访问量突然猛增到系统能承受的最大阀值，此时可以根据情况自动降级或者人工降级；
- 严重错误：比如因为特殊原因数据错误了，此时需要紧急人工降级。

# Linux

## 常用的命令

| 命令                   | 作用                                           |
| ---------------------- | ---------------------------------------------- |
| free -h                | 显示系统中空闲和使用的内存(-m标志表示内存(MB)) |
| df -h                  | 显示安装系统上的空闲空间                       |
| top                    | 显示所有正在运行的进程                         |
| ps aux \|grep ‘telnet’ | 搜索进程’telnet’的id                           |
| ifconfig               | 显示所有网络接口的IP地址                       |
| kill pid               | 使用给定的pid终止进程                          |
| tail file_name         | 显示文件的最后10行                             |
| lsof -i :              | 查看端口使用情况                               |

## 使用cat查看日志可能造成的问题

- 内存消耗：`cat` 命令会一次性读取整个文件并将其发送到标准输出（stdout），如果日志文件非常大，这可能会导致服务器的内存消耗增加，从而影响到其他服务的正常运行。
- 数据丢失：如果日志文件正在被其他进程实时追加记录，使用 `cat` 查看日志可能会错过最新的日志记录。

# RocketMQ

## 消息类型

| 类型          | 作用                                                         |
| ------------- | ------------------------------------------------------------ |
| 普通消息      | 支持生产者和消费者的异步解耦通信                             |
| 定时/延迟消息 | 消息被发送至服务端后，在指定时间后才能被消费者消费           |
| 顺序消息      | 支持消费者按照发送消息的先后顺序获取消息，从而实现业务场景中的顺序处理 |
| 事务消息      | 支持在分布式场景下保障消息生产和本地事务的最终一致性。       |

## 1. RocketMQ是什么？

RocketMQ 是阿里巴巴开源的分布式消息中间件。支持事务消息、顺序消息、批量消息、定时消息、消息回溯等。它里面有几个区别于标准消息中件间的概念，如Group、Topic、Queue等。系统组成则由Producer、Consumer、Broker、NameServer等。

**RocketMQ 特点**

- 是一个队列模型的消息中间件，具有高性能、高可靠、高实时、分布式等特点
- Producer、Consumer、队列都可以分布式
- Producer 向一些队列轮流发送消息，队列集合称为 Topic，Consumer 如果做广播消费，则一个 Consumer 实例消费这个 Topic 对应的所有队列，如果做集群消费，则多个 Consumer 实例平均消费这个 Topic 对应的队列集合
- 能够保证严格的消息顺序
- 支持拉（pull）和推（push）两种消息模式
- 高效的订阅者水平扩展能力
- 实时的消息订阅机制
- 亿级消息堆积能力
- 支持多种消息协议，如 JMS、OpenMessaging 等
- 较少的依赖

## 2. RocketMQ由哪些角色组成，每个角色作用和特点是什么？

| 角色       | 作用                                                         |
| ---------- | ------------------------------------------------------------ |
| Nameserver | 无状态，动态列表；这也是和zookeeper的重要区别之一。zookeeper是有状态的。 |
| Producer   | 消息生产者，负责发消息到Broker。                             |
| Broker     | 就是MQ本身，负责收发消息、持久化消息等。                     |
| Consumer   | 消息消费者，负责从Broker上拉取消息进行消费，消费完进行ack。  |

## 3. RocketMQ消费模式有几种？

消费模型由Consumer决定，消费维度为Topic。

1、集群消费

* 一条消息只会被同Group中的一个Consumer消费

* 多个Group同时消费一个Topic时，每个Group都会有一个Consumer消费到数据

2、广播消费

消息将对一 个Consumer Group 下的各个 Consumer 实例都消费一遍。即即使这些 Consumer 属于同一个Consumer Group ，消息也会被 Consumer Group 中的每个 Consumer 都消费一次。

## 4. RocketMQ消费消息是push还是pull？

RocketMQ没有真正意义的push，都是pull，虽然有push类，但实际底层实现采用的是**长轮询机制**，即拉取方式

> broker端属性 longPollingEnable 标记是否开启长轮询。默认开启

### 追问：为什么要主动拉取消息而不使用事件监听方式？

事件驱动方式是建立好长连接，由事件（发送数据）的方式来实时推送。

如果broker主动推送消息的话有可能push速度快，消费速度慢的情况，那么就会造成消息在consumer端堆积过多，同时又不能被其他consumer消费的情况。而pull的方式可以根据当前自身情况来pull，不会造成过多的压力而造成瓶颈。所以采取了pull的方式。

## 5. broker如何处理拉取请求的？

Consumer首次请求Broker

- Broker中是否有符合条件的消息

- 有

- - 响应Consumer
- 等待下次Consumer的请求

- 没有

- - DefaultMessageStore#ReputMessageService#run方法
- PullRequestHoldService 来Hold连接，每个5s执行一次检查pullRequestTable有没有消息，有的话立即推送
- 每隔1ms检查commitLog中是否有新消息，有的话写入到pullRequestTable
- 当有新消息的时候返回请求
- 挂起consumer的请求，即不断开连接，也不返回数据
- 使用consumer的offset，

## 6. 如何让RocketMQ保证消息的顺序消费？

首先多个queue只能保证单个queue里的顺序，queue是典型的FIFO，天然顺序。多个queue同时消费是无法绝对保证消息的有序性的。所以总结如下：

同一topic，同一个QUEUE，发消息的时候一个线程去发送消息，消费的时候 一个线程去消费一个queue里的消息。

## 重复消费问题的解决方案

一般情况下，我们会在消费者端解决重复消费的问题，消费者端需要进行去重，去重关键点就是要找到消息的唯一标记，所以我们在发送消息时会带有一个key，消费者拿到相同的key就不进行操作了。（一般使用redis存储消费过的key，或者使用mysql存储消费记录，把key设置成唯一索引。）

## 7. RocketMQ如何保证消息不丢失？

首先在如下三个部分都可能会出现丢失消息的情况：

- Producer端
- Broker端
- Consumer端

1 、Producer端如何保证消息不丢失

- 采取send()同步发消息，发送结果是同步感知的。
- 发送失败后可以重试，设置重试次数。默认3次。

- 集群部署，比如发送失败了的原因可能是当前Broker宕机了，重试的时候会发送到其他Broker上。

2、Broker端如何保证消息不丢失

- 修改刷盘策略为同步刷盘。默认情况下是异步刷盘的。

- 集群部署，主从模式，高可用。

3、Consumer端如何保证消息不丢失

- 完全消费正常后在进行手动ack确认。

## 7. rocketMQ的消息堆积如何处理？

首先要找到是什么原因导致的消息堆积，是Producer太多了，Consumer太少了导致的还是说其他情况，总之先定位问题。

然后看下消息消费速度是否正常，正常的话，可以通过上线更多consumer临时解决消息堆积问题

### 追问：如果Consumer和Queue不对等，上线了多台也在短时间内无法消费完堆积的消息怎么办？

- 准备一个临时的topic
- queue的数量是堆积的几倍
- queue分布到多Broker中
- 上线一台Consumer做消息的搬运工，把原来Topic中的消息挪到新的Topic里，不做业务逻辑处理，只是挪过去
- 上线N台Consumer同时消费临时Topic中的数据
- 改bug
- 恢复原来的Consumer，继续消费之前的Topic

### 追问：堆积时间过长消息超时了？

RocketMQ中的消息只会在commitLog被删除的时候才会消失，不会超时。也就是说未被消费的消息不会存在超时删除这情况。

### 追问：堆积的消息会不会进死信队列？

不会，消息在消费失败后会进入重试队列（%RETRY%+ConsumerGroup），18次（默认18次，网上所有文章都说是16次，无一例外。但是我没搞懂为啥是16次，这不是18个时间吗 ？）才会进入死信队列（%DLQ%+ConsumerGroup）。

## 8. RocketMQ为什么自研nameserver而不用zk？

1. RocketMQ只需要一个轻量级的维护元数据信息的组件，为此引入zk增加维护成本还强依赖另一个中间件了。
2. RocketMQ追求的是AP，而不是CP，也就是需要高可用。
    * zk是CP，因为zk节点间通过zap协议有数据共享，每个节点数据会一致，但是zk集群当挂了一半以上的节点就没法使用了。
    * nameserver是AP，节点间不通信，这样会导致节点间数据信息会发生短暂的不一致，但每个broker都会定时向所有nameserver上报路由信息和心跳。当某个broker下线了，nameserver也会延时30s才知道，而且不会通知客户端（生产和消费者），只能靠客户端自己来拉，rocketMQ是靠消息重试机制解决这个问题的，所以是最终一致性。但nameserver集群只要有一个节点就可用。https://juejin.cn/post/6844904068771479559

# Docker

```shell
# 列出本机的所有 image 文件。
$ docker image ls

# 删除 image 文件
$ docker image rm [imageName]

# 从 Docker 镜像仓库获取镜像的命令是 docker pull
$ docker pull [选项] [Docker Registry 地址[:端口号]/]仓库名[:标签]

===============================================================================
docker search 镜像id或name：在Docker Hub（或其他镜像仓库如阿里镜像）仓库中搜索关键字的镜像
docker pull 镜像id或name：从仓库中下载镜像，若要指定版本，则要在冒号后指定
docker images：列出已经下载的镜像，查看镜像
docker rmi 镜像id或name：删除本地镜像
docker rmi -f 镜像id或name:  删除镜像
docker build：构建镜像
```

```shell
# 列出本机正在运行的容器
$ docker container ls

# 列出本机所有容器，包括终止运行的容器
$ docker container ls --all

# 删除容器
$ docker container rm [containerID]

# docker container run命令会从 image 文件，生成一个正在运行的容器实例 
$ docker container run hello-world

# docker container kill 命令手动终止
$ docker container kill [containID]

===============================================================================
docker ps：列出运行中的容器
docker ps -a ： 查看所有容器，包括未运行
docker stop 容器id或name：停止容器
docker kill 容器id：强制停止容器
docker start 容器id或name：启动已停止的容器
docker inspect 容器id：查看容器的所有信息
docker container logs 容器id：查看容器日志
docker top 容器id：查看容器里的进程
docker exec -it 容器id /bin/bash：进入容器
exit：退出容器
docker rm 容器id或name：删除已停止的容器
docker rm -f 容器id：删除正在运行的容器
docker exec -it 容器ID sh :进入容器
```

## 容器是什么

​	容器是一种操作系统虚拟化技术，它允许你在同一台主机上运行多个隔离的应用程序。每个容器都是独立的，拥有自己的文件系统、网络接口和进程空间，共享宿主机的操作系统内核。

怎么打包 ARM 框架的镜像

​	Docker 打包的镜像是和宿主机一致的，要跨平台打包，需要做一下2点。

1. 选择指定适合的 ARM 框架的基础镜像。

2. 准备好了 Dockerfile 后，使用 `docker buildx` 命令来构建镜像。

   ```bash
   docker buildx build --platform=linux/arm/v7 -t your-repo:your-tag .
   ```

# 计算机通识 

## 设计模式

### 单例模式 (Singleton Pattern)

​	单例模式确保一个类只有一个实例，并提供一个全局访问点来访问这个实例。这在需要控制资源（如数据库连接、日志文件等）的情况下非常有用。

### 工厂模式 (Factory Pattern)

​	工厂模式是一种创建型设计模式，它提供了一种创建对象的最佳方式。工厂模式隐藏了对象创建的具体实现，使客户端不需要知道具体的创建细节。

### 策略模式

​	策略模式（Strategy Pattern）是行为设计模式中的一种，它定义了一系列的算法，并将每一个算法封装起来，使它们可以互相替换。这一模式让算法独立于使用它的客户而变化。



## 进程和线程的通信方式

- 进程
  - 消息队列（Message Queues）：这是一种高级形式的通信机制，它允许消息发送者将消息放入队列中，然后由接收者从队列中读取这些消息。
  - 共享内存（Shared Memory）：共享内存是最快的IPC机制之一，因为它涉及到了最少的数据拷贝。但是，使用共享内存需要同步机制来防止多个进程同时写入共享内存区域。
  - 套接字（Sockets）：通常用于网络通信，但也可以用于本地进程间通信。

- 线程

  - 互斥锁（Mutexes）：用来保护共享资源免受多线程并发访问的影响。

  - 条件变量（Condition Variables）：条件变量常与互斥锁一起使用，一个线程可以等待某个条件成立，而另一个线程可以通知这个条件成立。

  - 原子操作（Atomic Operations）：某些操作可以在单个处理器指令中完成，因此不会被其他线程中断。

# Java

## spring

### AOP

​	AOP是一种编程范式，旨在通过分离横切关注点（如日志记录、事务管理、安全性等）来提高模块化程度。这些横切关注点通常是跨多个模块的功能，如果分散在各个模块中处理，会导致代码重复和难以维护

### IoC

​	Inversion of Control，控制反转是一种设计原则，用于减少代码之间的依赖。在传统的编程模式中，应用程序本身负责创建并管理对象及其依赖关系。而在IoC模式下，这些职责被转移到了外部容器（如Spring容器）身上